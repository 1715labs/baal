{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Approximating posterior distributions from neural networks\n",
    "\n",
    "When we started developing active learning methods, we realised that what we wanted to\n",
    "achieve required estimating the uncertainty of models. Doing so for neural networks is\n",
    "an ongoing active research area.\n",
    "\n",
    "For the purposes of `baal`, we have implemented a few methods that are relatively generic\n",
    "and work with many neural networks.\n",
    "\n",
    "All the techniques implemented effectively produce approximate samples from the posterior.\n",
    "For classification techniques, this means that you usually end up with a 3D tensor rather\n",
    "than a 2D tensor (`n_batch x n_classes x n_samples` rather than `n_batch x n_classes`).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Monte-Carlo Dropout\n",
    "\n",
    "Monte-Carlo Dropout, or MC Dropout, is a very simple way of accessing uncertainty\n",
    "in a network that include Dropout layers. Essentially, rather than turning off\n",
    "dropout during inference, you keep in on and make multiple predictions on the\n",
    "same data. Due to the stochastic zeroing of weights, you'll get a different for\n",
    "every iteration, even if the input is the same.\n",
    "\n",
    "This is valid primarily because you trained the network using dropout: You have\n",
    "already learnt to make predictions without all the weights.\n",
    "\n",
    "The output is a distribution of predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Usage\n",
    "\n",
    "In order to use it, you can simply import Dropout layers from baal and use them in your model construction:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import baal.bayesian.dropout\n",
    "\n",
    "standard_model = torch.nn.Sequential(\n",
    "    torch.nn.Linear(10, 8),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Dropout(p=0.5),\n",
    "    torch.nn.Linear(8, 4),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Dropout(p=0.5),\n",
    "    torch.nn.Linear(4, 2),\n",
    ")\n",
    "\n",
    "mc_dropout_model = torch.nn.Sequential(\n",
    "    torch.nn.Linear(10, 8),\n",
    "    torch.nn.ReLU(),\n",
    "    baal.bayesian.dropout.Dropout(p=0.5),\n",
    "    torch.nn.Linear(8, 4),\n",
    "    torch.nn.ReLU(),\n",
    "    baal.bayesian.dropout.Dropout(p=0.5),\n",
    "    torch.nn.Linear(4, 2),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main difference between these is that the standard model will set the dropout probability to zero during eval, while the MC dropout model will not:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "dummy_input = torch.randn(8, 10)\n",
    "\n",
    "standard_model.eval()\n",
    "print(bool((standard_model(dummy_input) == standard_model(dummy_input)).all()))\n",
    "\n",
    "mc_dropout_model.eval()\n",
    "print(bool((mc_dropout_model(dummy_input) == mc_dropout_model(dummy_input)).all()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to get a distribution of model outputs, you simply need to repeatedly run the same data through the MC Dropout model. `baal` makes this easier for you by providing a class called `ModelWrapper`. This class accepts your model and a criterion (loss) function, and provides several utility functions, such as running training steps and more. The one that is important for obtaining a posterior distribution is `Modelwrapper.predict_on_batch`.\n",
    "\n",
    "This method allows you to specify a number of iterations to run the model for, and produces a distribution accordingly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from baal.modelwrapper import ModelWrapper\n",
    "\n",
    "wrapped_model = ModelWrapper(\n",
    "    mc_dropout_model,\n",
    "    torch.nn.MSELoss()\n",
    ")\n",
    "\n",
    "with torch.no_grad():\n",
    "    predictions = wrapped_model.predict_on_batch(dummy_input, iterations=10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tensor \"prediction_distribution\" has the shape (batch size) x (output size) x iterations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 2, 10000])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can visualise this posterior distribution, for example for the first data point in our\n",
    "minibatch (although note that because this model is overly simplistic, this is not very\n",
    "useful):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.hist(predictions[0, 0, :].numpy(), bins=50);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using stochastic weight averaging\n",
    "\n",
    "Stochastic weight averaging with gaussian modelling (SWAG) is a method to approximate posteriors by sampling\n",
    "the weights of a neural network from a gaussian distribution that is fitted to samples from the stochastic\n",
    "gradient descent iterates.\n",
    "\n",
    "We implement this in `baal` as an optimiser, since the samples are taken during the optimisation steps.\n",
    "\n",
    "This optimiser class is in the `swag` submodule. It accepts all parameters as the standard SGD optimiser\n",
    "in pytorch, and in addition accepts three parameters to determine the SWAG behaviour:\n",
    "\n",
    "- `swa_burn_in`, or how many steps of SGD optimisation are taken before SWAG samples are collected\n",
    "- `swa_steps`, or how many steps (i.e. mini-batches) to take in between SWAG samples\n",
    "- `n_deviations`, or how many of the most recent deviations to use to fit the gaussian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from baal import swag\n",
    "\n",
    "optimiser = swag.StochasticWeightAveraging(\n",
    "    standard_model.parameters(),\n",
    "    lr=1e-3,\n",
    "    weight_decay=1e-5,\n",
    "    swa_burn_in=100,\n",
    "    swa_steps=20,\n",
    "    n_deviations=20,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you use this optimiser to fit your neural network, the optimiser will collect the mean and\n",
    "variation of each of the network's weights every 20 SGD steps, following the first 100 SGD steps.\n",
    "\n",
    "_Usually_, you should set `swa_burn_in` to upwards of tens of epochs, and swa_steps to on the\n",
    "order of one epoch. Note that the optimiser is unaware of the length of an epoch, and so you\n",
    "need to specify the amounts in terms of SGD steps, or mini-batches.\n",
    "\n",
    "For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from baal import swag\n",
    "import torch\n",
    "import torch.utils.data\n",
    "\n",
    "# the data shape:\n",
    "batch_size = 8\n",
    "dataset_size = 20 * 8\n",
    "# create some dummy data:\n",
    "x = torch.randn(dataset_size, 10)\n",
    "y = torch.randint(low=0, high=2, size=(dataset_size,)).long()\n",
    "# write loaders for these:\n",
    "dummy_dataset = torch.utils.data.TensorDataset(x, y)\n",
    "dummy_loader = torch.utils.data.DataLoader(dummy_dataset, batch_size=batch_size)\n",
    "# write a simple model:\n",
    "model = torch.nn.Sequential(\n",
    "    torch.nn.Linear(10, 5),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Dropout(p=0.5),\n",
    "    torch.nn.Linear(5, 2),\n",
    ")\n",
    "criterion = torch.nn.NLLLoss()\n",
    "# create the SWAG optimiser:\n",
    "optimiser = swag.StochasticWeightAveraging(\n",
    "    model.parameters(),\n",
    "    lr=1e-3,\n",
    "    weight_decay=1e-5,\n",
    "    # burn in for 50 epochs:\n",
    "    swa_burn_in=50*len(dummy_loader),\n",
    "    # then collect samples every epoch:\n",
    "    swa_steps=len(dummy_loader),\n",
    "    n_deviations=20,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then train our model as normal:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(75):\n",
    "\n",
    "    for x, y in dummy_loader:\n",
    "\n",
    "        loss = criterion(model(x), y)\n",
    "        loss.backward()\n",
    "        optimiser.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to obtain uncertainty estimates for our predictions, we need to \"sample\" models\n",
    "multiple times and make predictions with each model separately.\n",
    "\n",
    "For example, if we want to sample from the approximate posterior 100 times we do:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_batch = torch.randn(8, 10)\n",
    "\n",
    "model.eval()\n",
    "\n",
    "predictions = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for n in range(1000):\n",
    "        optimiser.sample()\n",
    "        prediction = model(eval_batch)\n",
    "        predictions.append(prediction)\n",
    "\n",
    "predictions = torch.stack(predictions, dim=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, this can be simplified by using the `swag.ModelWrapper` class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_wrapper = swag.SwagModelWrapper(\n",
    "    model,\n",
    "    criterion\n",
    ")\n",
    "\n",
    "with torch.no_grad():\n",
    "    predictions = model_wrapper.predict_on_batch(\n",
    "        eval_batch,\n",
    "        optimiser,\n",
    "        iterations=1000\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have 1000 predictions for every data point in our mini-batch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 2, 1000])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we can visualise the posterior:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAEJCAYAAACaFuz/AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvOIA7rQAAEptJREFUeJzt3X+MXWd95/H3p04ICGgDZJr12qaOqFuURsWg2ZCWqsompTWhwqECFHYFXprKRZtIINiWpK1EqRopVbekoNJUbpNiViwh5YdiQfrDDUFspJJ0Qo1JYlhmISi2TDyFJJAisnX49o/7uB2lY8+9c+/1tR/eL2k05zznOed8jzz+zJnnnh+pKiRJ/fqBWRcgSZoug16SOmfQS1LnDHpJ6pxBL0mdM+glqXMGvSR1zqCXpM4Z9JLUuTNmXQDAOeecU5s3b551GZJ0Wrn33nv/sarmVut3SgT95s2bWVhYmHUZknRaSfK1Yfo5dCNJnTPoJalzBr0kdW7ooE+yLsk/JPlEmz8vyd1JFpN8OMnTWvtZbX6xLd88ndIlScMY5Yz+LcCBZfO/B9xQVT8KPAJc2dqvBB5p7Te0fpKkGRkq6JNsBF4J/FmbD3AJ8JHWZTdweZve3uZpyy9t/SVJMzDsGf0fAr8OfK/NPw94tKqOtvmDwIY2vQF4CKAtf6z1lyTNwKpBn+QXgSNVde8kd5xkZ5KFJAtLS0uT3LQkaZlhzuhfBrwqyYPALQyGbN4DnJ3k2A1XG4FDbfoQsAmgLf8h4BtP3WhV7aqq+aqan5tb9cYuSdIarXpnbFVdC1wLkORi4H9U1X9N8hfAaxiE/w7gtrbKnjb/d235p8o3kOsUsfmaTx532YPXv/IkViKdPONcR/8O4G1JFhmMwd/U2m8Cntfa3wZcM16JkqRxjPSsm6r6NPDpNv0V4MIV+nwXeO0EapMkTYB3xkpS5wx6SeqcQS9JnTPoJalzBr0kdc6gl6TOGfSS1DmDXpI6Z9BLUucMeknqnEEvSZ0z6CWpcwa9JHXOoJekzhn0ktQ5g16SOmfQS1LnDHpJ6tyqQZ/k6UnuSfL5JPcneVdrf3+SrybZ1762tvYkeW+SxST7k7xk2gchSTq+Yd4Z+wRwSVU9nuRM4K4kf9mW/VpVfeQp/V8BbGlfLwVubN8lSTOw6hl9DTzeZs9sX3WCVbYDH2jrfRY4O8n68UuVJK3FUGP0SdYl2QccAfZW1d1t0XVteOaGJGe1tg3AQ8tWP9janrrNnUkWkiwsLS2NcQiSpBMZKuir6smq2gpsBC5McgFwLfBC4D8BzwXeMcqOq2pXVc1X1fzc3NyIZUuShjXSVTdV9ShwJ7Ctqg634ZkngD8HLmzdDgGblq22sbVJkmZgmKtu5pKc3aafAbwc+OKxcfckAS4H7mur7AHe2K6+uQh4rKoOT6V6SdKqhrnqZj2wO8k6Br8Ybq2qTyT5VJI5IMA+4M2t/+3AZcAi8B3gTZMvW5I0rFWDvqr2Ay9eof2S4/Qv4KrxS5MkTYJ3xkpS5wx6SeqcQS9JnTPoJalzBr0kdc6gl6TOGfSS1DmDXpI6Z9BLUucMeknqnEEvSZ0z6CWpcwa9JHXOoJekzhn0ktQ5g16SOmfQS1Lnhnln7NOT3JPk80nuT/Ku1n5ekruTLCb5cJKntfaz2vxiW755uocgSTqRYc7onwAuqaoXAVuBbe2l378H3FBVPwo8AlzZ+l8JPNLab2j9JEkzsmrQ18DjbfbM9lXAJcBHWvtu4PI2vb3N05ZfmiQTq1iSNJKhxuiTrEuyDzgC7AX+H/BoVR1tXQ4CG9r0BuAhgLb8MeB5K2xzZ5KFJAtLS0vjHYUk6biGCvqqerKqtgIbgQuBF46746raVVXzVTU/Nzc37uYkSccx0lU3VfUocCfwU8DZSc5oizYCh9r0IWATQFv+Q8A3JlKtJGlkw1x1M5fk7Db9DODlwAEGgf+a1m0HcFub3tPmacs/VVU1yaIlScM7Y/UurAd2J1nH4BfDrVX1iSQPALck+V3gH4CbWv+bgP+VZBH4JnDFFOqWJA1p1aCvqv3Ai1do/wqD8fqntn8XeO1EqpMkjc07YyWpcwa9JHXOoJekzhn0ktQ5g16SOmfQS1LnDHpJ6pxBL0mdM+glqXMGvSR1zqCXpM4Z9JLUOYNekjpn0EtS5wx6SeqcQS9JnTPoJalzw7wzdlOSO5M8kOT+JG9p7b+d5FCSfe3rsmXrXJtkMcmXkvzCNA9AknRiw7wz9ijw9qr6XJJnA/cm2duW3VBV/3N55yTnM3hP7E8A/xH42yQ/VlVPTrJwSdJwVj2jr6rDVfW5Nv1t4ACw4QSrbAduqaonquqrwCIrvFtWknRyjDRGn2QzgxeF392ark6yP8nNSZ7T2jYADy1b7SAr/GJIsjPJQpKFpaWlkQuXJA1n6KBP8izgo8Bbq+pbwI3AC4CtwGHgD0bZcVXtqqr5qpqfm5sbZVVJ0giGCvokZzII+Q9W1ccAqurhqnqyqr4H/Cn/NjxzCNi0bPWNrU2SNAPDXHUT4CbgQFW9e1n7+mXdXg3c16b3AFckOSvJecAW4J7JlSxJGsUwV928DHgD8IUk+1rbbwCvT7IVKOBB4FcBqur+JLcCDzC4Yucqr7iRpNlZNeir6i4gKyy6/QTrXAdcN0ZdkqQJ8c5YSeqcQS9JnTPoJalzBr0kdc6gl6TOGfSS1DmDXpI6Z9BLUucMeknqnEEvSZ0z6CWpcwa9JHXOoJekzhn0ktQ5g16SOmfQS1LnDHpJ6tww74zdlOTOJA8kuT/JW1r7c5PsTfLl9v05rT1J3ptkMcn+JC+Z9kFIko5vmDP6o8Dbq+p84CLgqiTnA9cAd1TVFuCONg/wCgYvBN8C7ARunHjVkqShrRr0VXW4qj7Xpr8NHAA2ANuB3a3bbuDyNr0d+EANfBY4O8n6iVcuSRrKSGP0STYDLwbuBs6tqsNt0deBc9v0BuChZasdbG1P3dbOJAtJFpaWlkYsW5I0rKGDPsmzgI8Cb62qby1fVlUF1Cg7rqpdVTVfVfNzc3OjrCpJGsFQQZ/kTAYh/8Gq+lhrfvjYkEz7fqS1HwI2LVt9Y2uTJM3AMFfdBLgJOFBV7162aA+wo03vAG5b1v7GdvXNRcBjy4Z4JEkn2RlD9HkZ8AbgC0n2tbbfAK4Hbk1yJfA14HVt2e3AZcAi8B3gTROtWJI0klWDvqruAnKcxZeu0L+Aq8asS5I0Id4ZK0mdM+glqXMGvSR1zqCXpM4Z9JLUOYNekjpn0EtS5wx6SeqcQS9JnTPoJalzBr0kdc6gl6TOGfSS1DmDXpI6Z9BLUucMeknqnEEvSZ0b5p2xNyc5kuS+ZW2/neRQkn3t67Jly65NspjkS0l+YVqFS5KGM8wZ/fuBbSu031BVW9vX7QBJzgeuAH6irfPHSdZNqlhJ0uhWDfqq+gzwzSG3tx24paqeqKqvMnhB+IVj1CdJGtM4Y/RXJ9nfhnae09o2AA8t63OwtUmSZmStQX8j8AJgK3AY+INRN5BkZ5KFJAtLS0trLEOStJo1BX1VPVxVT1bV94A/5d+GZw4Bm5Z13djaVtrGrqqar6r5ubm5tZQhSRrCmoI+yfpls68Gjl2Rswe4IslZSc4DtgD3jFeiJGkcZ6zWIcmHgIuBc5IcBN4JXJxkK1DAg8CvAlTV/UluBR4AjgJXVdWT0yldkjSMVYO+ql6/QvNNJ+h/HXDdOEVJkibHO2MlqXMGvSR1zqCXpM4Z9JLUOYNekjpn0EtS5wx6SeqcQS9JnTPoJalzBr0kdW7VRyBI3y82X/PJFdsfvP6VJ7kSabI8o5ekzhn0ktQ5g16SOmfQS1LnDHpJ6pxBL0mdWzXok9yc5EiS+5a1PTfJ3iRfbt+f09qT5L1JFpPsT/KSaRYvSVrdMGf07we2PaXtGuCOqtoC3NHmAV7B4IXgW4CdwI2TKVOStFarBn1VfQb45lOatwO72/Ru4PJl7R+ogc8CZydZP6liJUmjW+sY/blVdbhNfx04t01vAB5a1u9ga5MkzcjYH8ZWVQE16npJdiZZSLKwtLQ0bhmSpONYa9A/fGxIpn0/0toPAZuW9dvY2v6dqtpVVfNVNT83N7fGMiRJq1lr0O8BdrTpHcBty9rf2K6+uQh4bNkQjyRpBlZ9emWSDwEXA+ckOQi8E7geuDXJlcDXgNe17rcDlwGLwHeAN02hZknSCFYN+qp6/XEWXbpC3wKuGrcoSdLkeGesJHXOoJekzhn0ktQ5XyUorcJXDOp05xm9JHXOoJekzhn0ktQ5g16SOmfQS1LnDHpJ6pxBL0mdM+glqXMGvSR1zqCXpM4Z9JLUOZ91I62Rz8DR6cIzeknq3Fhn9EkeBL4NPAkcrar5JM8FPgxsBh4EXldVj4xXpiRprSZxRv+fq2prVc23+WuAO6pqC3BHm5ckzcg0xui3M3iZOMBu4NPAO6awH+m4jjd+Ln0/GveMvoC/SXJvkp2t7dyqOtymvw6cO+Y+JEljGPeM/meq6lCSHwb2Jvni8oVVVUlqpRXbL4adAM9//vPHLEOSdDxjndFX1aH2/QjwceBC4OEk6wHa9yPHWXdXVc1X1fzc3Nw4ZUiSTmDNQZ/kmUmefWwa+HngPmAPsKN12wHcNm6RkqS1G2fo5lzg40mObed/V9VfJfl74NYkVwJfA143fpmSpLVac9BX1VeAF63Q/g3g0nGKkiRNjnfGSlLnDHpJ6pxBL0mdM+glqXMGvSR1zufR67R2Kj7TxufU61TjGb0kdc6gl6TOOXSjU4rDHtLkeUYvSZ3zjF6asVE/UPavG43KoNdp4VS8umZWHN7SqAx66STxl5VmxTF6SeqcZ/RSJxzS0fEY9JoJhzGkk8eg10R4Nnnq8t9GUwv6JNuA9wDrgD+rquuntS9N3qTCwTP3U9da/o39pXF6msqHsUnWAe8DXgGcD7w+yfnT2Jck6cSmdUZ/IbDY3itLkluA7cADU9qfThLP0Pu3ln9jb/o6tU0r6DcADy2bPwi8dBo7OtEP2KR+mKb95+q0h0nW8qe4NE2T+rk73s/2tLe/FrMc9kpVTX6jyWuAbVX1K23+DcBLq+rqZX12Ajvb7I8DX5p4ISfHOcA/zrqIKev9GHs/PvAYe/HUY/yRqppbbaVpndEfAjYtm9/Y2v5VVe0Cdk1p/ydNkoWqmp91HdPU+zH2fnzgMfZircc4rTtj/x7YkuS8JE8DrgD2TGlfkqQTmMoZfVUdTXI18NcMLq+8uarun8a+JEknNrXr6KvqduD2aW3/FHLaDz8Nofdj7P34wGPsxZqOcSofxkqSTh0+vVKSOmfQT0CS30/yxST7k3w8ydmzrmmSkrw2yf1Jvpekq6sakmxL8qUki0mumXU9k5bk5iRHktw361qmIcmmJHcmeaD9jL5l1jVNWpKnJ7knyefbMb5r1G0Y9JOxF7igqn4S+L/AtTOuZ9LuA34J+MysC5mk75NHdbwf2DbrIqboKPD2qjofuAi4qsN/wyeAS6rqRcBWYFuSi0bZgEE/AVX1N1V1tM1+lsF9A92oqgNVdbre0HYi//qojqr6/8CxR3V0o6o+A3xz1nVMS1UdrqrPtelvAwcY3JnfjRp4vM2e2b5G+nDVoJ+8Xwb+ctZFaCgrPaqjq5D4fpJkM/Bi4O7ZVjJ5SdYl2QccAfZW1UjH6PPoh5Tkb4H/sMKi36yq21qf32Twp+QHT2ZtkzDM8UmnqiTPAj4KvLWqvjXreiatqp4EtrbP/z6e5IKqGvpzF4N+SFX1cydanuS/Ab8IXFqn4TWrqx1fp1Z9VIdOfUnOZBDyH6yqj826nmmqqkeT3Mngc5ehg96hmwloL1n5deBVVfWdWdejofmojtNckgA3AQeq6t2zrmcakswdu5IvyTOAlwNfHGUbBv1k/BHwbGBvkn1J/mTWBU1SklcnOQj8FPDJJH8965omoX2AfuxRHQeAW3t7VEeSDwF/B/x4koNJrpx1TRP2MuANwCXt/96+JJfNuqgJWw/cmWQ/g5OTvVX1iVE24J2xktQ5z+glqXMGvSR1zqCXpM4Z9JLUOYNekqZklIfKJXlzki+0K4fuOvbMniTPaw9uezzJH62pDq+6kaTpSPKzwOPAB6rqglX6/uCxu3qTvAr471W1LckzGTza4QIGD0+8etQ6PKOXpClZ6aFySV6Q5K+S3Jvk/yR5Yeu7/NENz6Q9uKyq/qmq7gK+u9Y6fASCJJ1cu4A3V9WXk7wU+GPgEoAkVwFvA552rG0SDHpJOknaw9d+GviLwdMbADjr2ERVvQ94X5L/AvwWsGMS+zXoJenk+QHg0araukq/W4AbJ7lTSdJJ0Mbhv5rktTB4KFuSF7XpLcu6vhL48qT261U3kjQl7aFyFwPnAA8D7wQ+xeBsfT2Dt0XdUlW/k+Q9wM8B/ww8Alx97CF7SR4EfpDB2P2jwM9X1QND12HQS1LfHLqRpM4Z9JLUOYNekjpn0EtS5wx6SeqcQS9JnTPoJalzBr0kde5fACFBO/5E39m6AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.hist(predictions[0, 0, :].numpy(), bins=50);"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
